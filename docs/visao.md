Este repositório apresenta a solução proposta para o Desafio de Engenharia de Dados Júnior CBLab.

O desafio tem como foco testar a capacidade do candidato de construção de esquema de SQL, compreensão da necessidade do negócio, Python, orquestrar fluxos com Apache Airflow, processos de ETL/ELT com data warehouses e/ou data lakes, manipulação de APIs, versionar no Git, e documentar todo o caminho percorrido por meio de um Kanban visível e de relatórios técnicos claros. 

Nesse desafio foi disponibilizado um arquivo JSON com as informações de um pedido/cheque retirado do ERP da rede, neste arquivo é possível observar informações sobre um guest check a "conta" do cliente identificada pela chave guestCheckId. Dentro dele encontramos dados de cabeçalho (datas de abertura e fechamento, valores agregados, mesa, garçom) e um array de detailLines, cada elemento possuí um guestCheckLineItemId e quando é um item do menu também tem um objeto menuItem que possuí atributos como taxa, desconto, taxas de serviço, meios de pagamento ou até códigos de erro.

O desafio é partido em duas partes complementares, sendo a primeira parte sobre analisar o arquivo JSON e descrever o seu esquema, transcrever o JSON para tabelas SQL conforme as necessidades de negócio de um restaurante e para análises de BI: um pedido pode ter muitos itens, cada item pode receber um ou mais descontos (Presunção feita pelo projeto), um pedido pode ser quitado por vários meios de pagamento (Presunção feita pelo projeto), e assim por diante. É preciso justificar cada decisão como tipos numéricos, chaves primárias, etc.

A segunda parte tem como foco o escopo para o cenário de produção. Devemos armazenar as respostas da API em um datalake para manipulação, verificações, buscas e pesquisas rápidas. Para isso iremos seguir um fluxo Medallion pegamos o dado bruto e gradualmente aumentamos o refinamento, guardamos o JSON bruto organizado por data e loja, depois transformamos e carregamos as tabelas finais. Tudo isso feito pelo um DAG no Apache Airflow, permitindo agendamento, monitoramento e reprocessamento caso haja mudanças no nome de atributos no esquema (guestChecks.taxes -> guestChecks.taxation).

A solução entregue aqui resolve ambas as frentes: apresenta o script Python que parseia o ERP.json e alimenta o banco; disponibiliza o DDL completo das tabelas; traz o DAG Airflow parametrizado por data; organiza o data lake local em pastas particionadas; e inclui testes Pytest, Docker Compose para subir todo o ambiente com um único comando e documentação passo a passo no README. O Kanban feito no próprio GitHub Projects. Assim o projeto segua a lógica incremental de design -> código -> teste -> documentação.